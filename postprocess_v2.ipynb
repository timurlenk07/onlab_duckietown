{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEOpc9vVaidZ"
   },
   "source": [
    "### Preprocess raw data\n",
    "This script processes the original and annotated video files exported from the simulator. The original and annotated file naming scheme takes the form of:\n",
    "\n",
    "(sequence_nr)_(orig|annot).avi\n",
    "\n",
    "One data point consist of an original, RGB image sequence (what the Duckiebot \"saw\") and a matching annotated, binary image sequence (right lane highlighted). To perform preprocessing of raw data we did the following:\n",
    " - On the original pictures: convert from BGR (OpenCV default) to RGB\n",
    " - On the annotated pictures: binarize the colored image by either applying a color threshold (less reliable) or substracting the original image from the annotated one (best performance) then convert to grayscale and threshold to binarize.\n",
    "\n",
    "For more info on how binarization (and data collection) works, please refer to [this](https://github.com/DeepTesla/deep_learning_hf/wiki/Data-used-for-training,-testing-and-validating-the-network) wiki page.\n",
    "\n",
    "The results of the script are preprocessed AND separated video files according to the following logic:\n",
    " - the first [1-test_ratio-valid_ratio] size of the data goes to the /training directory\n",
    " - the following [valid_ratio] size of the data goes to the /validation directory\n",
    " - the last [test_ratio] size of the data goes to the /test directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "To accomplish this task an installed OpenCV is required.\n",
    "\n",
    "We will also import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kx27udJmb6Aw"
   },
   "outputs": [],
   "source": [
    "# install the dependencies for cloud/notebook environments\n",
    "!pip install -q opencv-python>=3.4\n",
    "\n",
    "# import required libraries\n",
    "import glob\n",
    "import sys\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization algorithm\n",
    "We have developed two distinctly different binarization algorithm for the annotated image preprocessing.\n",
    "\n",
    "binarize_a() is the function where binarization is done using a color threshold in HSV space but, as this method seemed to give less reliable and \"nice\" result, it wasn't polished further.\n",
    "\n",
    "binarize_b() is the function where we substract the original image from the annotated one, then convert to grayscale and apply threshold. As the two images differ only in the right lane, this gave astonishing results and was used in further calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarization algorithm, with a given original and annotated pair of image\n",
    "def binarize_a(img_orig, img_ant):\n",
    "    \n",
    "    img_hsv = cv2.cvtColor(img_ant,cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lowerBound = (10, 0, 0); #HSV\n",
    "    upperBound = (170, 255, 255); #HSV\n",
    "    \n",
    "    mask = cv2.inRange(img_hsv, lowerBound, upperBound)\n",
    "    mask = ~mask\n",
    "    \n",
    "    result = mask\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(7,7))\n",
    "    result = cv2.morphologyEx(result, cv2.MORPH_OPEN, kernel)\n",
    "    result = cv2.morphologyEx(result, cv2.MORPH_CLOSE, kernel)\n",
    "    return result\n",
    "\n",
    "def binarize_b(img_orig, img_ant):\n",
    "    img_diff = img_orig - img_ant\n",
    "    \n",
    "    res_gray = cv2.cvtColor(img_diff,cv2.COLOR_BGR2GRAY)\n",
    "    res_gray[res_gray > 0] = 255\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))\n",
    "    result = cv2.morphologyEx(res_gray, cv2.MORPH_OPEN, kernel)\n",
    "    result = cv2.morphologyEx(result, cv2.MORPH_CLOSE, kernel)\n",
    "    return result\n",
    "\n",
    "# Wrapper function variable; select here the one you want to use\n",
    "binarize = binarize_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try to obtain the list of video files in the /recordings directory. While iterating through every pair of them we perform preprocessing on every frame then save them into a new video file under a subdirectory. The subdirectory name depends on the test_ratio and valid_ratio variables, as these decide the ratio of the videos that belongs to the train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Q2oU_Zkaida"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation/Testing dataset ratio set to: 6:2:2\n",
      "Processing recording nr. 0...\n",
      "Processing of recording nr. 0 done.\n",
      "Processing recording nr. 1...\n",
      "Processing of recording nr. 1 done.\n",
      "Processing recording nr. 2...\n",
      "Processing of recording nr. 2 done.\n",
      "Processing recording nr. 3...\n",
      "Processing of recording nr. 3 done.\n",
      "Post-processing finished!\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 6\n",
    "validation_ratio = 2\n",
    "test_ratio = 2\n",
    "total_ratio = train_ratio + validation_ratio + test_ratio\n",
    "print(\"Training/Validation/Testing dataset ratio set to: {}:{}:{}\".format(train_ratio, validation_ratio, test_ratio))\n",
    "\n",
    "# Get the list of available recordings\n",
    "annot_raw_list = glob.glob('recordings\\*_annot.avi')\n",
    "orig_raw_list = glob.glob('recordings\\*_orig.avi')\n",
    "\n",
    "# Check whether original and annotated recordings number match or not\n",
    "if len(annot_raw_list) != len(orig_raw_list):\n",
    "    print(\"Length mismatch! No postprocess performed.\")\n",
    "    sys.exit()\n",
    "\n",
    "unit_ratio = float(len(orig_raw_list) / total_ratio)\n",
    "# standardizing ratios\n",
    "train_ratio = int(train_ratio * unit_ratio)\n",
    "validation_ratio = int(validation_ratio * unit_ratio)\n",
    "test_ratio = int(test_ratio * unit_ratio)\n",
    "ratio_cnt = 0\n",
    "# Iterate and postprocess every recording\n",
    "for i in range(len(orig_raw_list)):\n",
    "    dir = \"\"\n",
    "    # the first train_ratio/unit_ratio video goes under the data/train folder\n",
    "    if ratio_cnt < train_ratio:\n",
    "        dir = os.path.join(os.getcwd(), \"data\", \"train\")\n",
    "    elif ratio_cnt < train_ratio + validation_ratio:\n",
    "        dir = os.path.join(os.getcwd(), \"data\", \"validation\")\n",
    "    else:\n",
    "        dir = os.path.join(os.getcwd(), \"data\", \"test\")\n",
    "    ratio_cnt += 1\n",
    "\n",
    "    # Open recordings...\n",
    "    cap_orig = cv2.VideoCapture(orig_raw_list[i])\n",
    "    cap_annot = cv2.VideoCapture(annot_raw_list[i])\n",
    "    if not cap_orig.isOpened() or not cap_annot.isOpened():\n",
    "        print(\"Could not open files! Continuing...\")\n",
    "        continue\n",
    "    \n",
    "    # Check whether recordings hold the same number of frames\n",
    "    if cap_orig.get(cv2.CAP_PROP_FRAME_COUNT) != cap_annot.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        print(\"Different video length encountered! Continuing...\")\n",
    "        print(\"DEBUG: orig frames: %i, annot frames: %i\" % (cap_orig.get(cv2.CAP_PROP_FRAME_COUNT), cap_annot.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        continue\n",
    "    \n",
    "    # Open VideoWriter Objects\n",
    "    fourcc=cv2.VideoWriter_fourcc(*'FFV1')\n",
    "    fps=20\n",
    "    framesize=(640,480)\n",
    "    isColor=True\n",
    "    # split the ./recordings/00000_orig.avi into 2 components (head, tail)\n",
    "    recordings_dir_path, filename_orig = os.path.split(orig_raw_list[i])\n",
    "    # further split the ./recordings dir to find the project root\n",
    "    project_root, _ = os.path.split(recordings_dir_path)\n",
    "    filename_orig, _ = os.path.splitext(filename_orig)\n",
    "    filename_orig = filename_orig + '_pp.avi'\n",
    "    if os.path.exists(filename_orig):   # If file exists...\n",
    "        os.remove(filename_orig)    # ...delete it\n",
    "    vWriter_orig = cv2.VideoWriter(os.path.join(dir, filename_orig), fourcc, fps, framesize, isColor)\n",
    "    \n",
    "    isColor=False\n",
    "    _, filename_annot = os.path.split(annot_raw_list[i])\n",
    "    filename_annot, _ = os.path.splitext(filename_annot)\n",
    "    filename_annot = filename_annot + '_pp.avi'\n",
    "    if os.path.exists(filename_annot):  # If file exists...\n",
    "        os.remove(filename_annot)   # ...delete it\n",
    "    vWriter_annot = cv2.VideoWriter(os.path.join(dir, filename_annot), fourcc, fps, framesize, isColor)\n",
    "    \n",
    "    if not vWriter_orig.isOpened() or not vWriter_annot.isOpened():\n",
    "        print(\"Could not open vide writers! Continuing...\")\n",
    "        vWriter_annot.release()\n",
    "        vWriter_orig.release()\n",
    "        continue\n",
    "    \n",
    "    # Produce output videos\n",
    "    print(\"Processing recording nr. {}...\".format(i))\n",
    "    while cap_orig.isOpened() and cap_annot.isOpened(): # Iterate through every frame\n",
    "        ret_o, frame_o = cap_orig.read()\n",
    "        ret_a, frame_a = cap_annot.read()\n",
    "        if not ret_o or not ret_a:\n",
    "            break\n",
    "\n",
    "        # Postprocess original recording: convert from BGR to RGB\n",
    "        vWriter_orig.write(cv2.cvtColor(frame_o,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Postprocess annotated frame: binarize it\n",
    "        annot_binary = binarize(frame_o, frame_a)\n",
    "        vWriter_annot.write(annot_binary)\n",
    "\n",
    "    \n",
    "    print(\"Processing of recording nr. {} done.\".format(i))\n",
    "    \n",
    "    # Release writer resources\n",
    "    vWriter_annot.release()\n",
    "    vWriter_orig.release()\n",
    "\n",
    "print(\"Post-processing finished!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Codes_and_comments.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
