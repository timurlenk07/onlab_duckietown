# Right Lane segmentation in Duckietown environment
Repository for private project Duckietown at BME VIK (Budapesti Műszaki és Gazdaságtudományi Egyetem Villamosmérnöki és Informatikai Kar).

All used packages are listed in requirements.txt.
Note however, that this module may work with other setup as well.

All the code related to neural networks are implemented in PyTorch and, recently, in PyTorch Lightning.
Versions are always updated, so I plan to use the newest version possible.
If this repo is finalised, or I wish to make a tag of it then I will add specific version informations.

## Prerequisites
All data generation code was run on Ubuntu 20 and Python3.8, while all training code was run on Ubuntu 16 and Python3.7 on a DGX Station.
For package requirements please see [requirements.txt](requirements.txt)

## Data generation
Data can be manually generated from the modified simulator contained in [the rightLaneDatagen folder](rightLaneDatagen).
To use it one needs to launch [manual_control.py](rightLaneDatagen/manual_control.py) with a selected map from [rightLaneDatagen/gym_duckietown/maps](rightLaneDatagen/gym_duckietown/maps).

Key bindings:
- Press 'A' to change annotated lane (right/left/none)
- Press 'Enter' to start recording (note: annotated lane should be selected first)
- Press 'Enter' again to stop the recording
- Press 'Backspace' in order to random reset the environment
- Press 'Q' to quit

Additional to selecting a map one may specifiy to use domain randomization with the _domain_rand_ flag set to True.
Another flag worthy of note is _distortion_ that allows camera distortion in domain randomization.

Saving of recorded videos is done via a background thread.
In case of annotation mode change or any reset condition met recording stops automatically.
Recorded video files are located in the created _recordings_ folder.

### Data postprocessing
As the saved videos are _NOT_ ready for training a simple preprocessing script, [postprocess_v2.py](rightLaneDatagen/postprocess_v2.py) helps converting the annotated RGB video to a binary, label-like video.
The post-processed data (now ready for training) is generated by default at _data_ directory.

The post-processing is basically a difference-of-images calculation, followed by binarization and morphological closing and opening.

See postprocess_v2.py for arguments and details.

### Example training data
Data pair 1:

![Original](doc/res_readme/orig_1.jpg)
![Annotated](doc/res_readme/annot_1.jpg)

Data pair 2:

![Original](doc/res_readme/orig_2.jpg)
![Annotated](doc/res_readme/annot_2.jpg)

## Training
### Baseline: Simulator-only training
#### Database formatting
We assume that the obtained video-label pairs in our database are structured in the following directory format:
```
simData
├── input
└── label
```

Run [preprocessDatabase.py](rightLaneNetwork/utils/preprocessDatabase.py) in order to disassemble the videos into separate images while also sample them into train, validation and test subsets.
A typical command would look like this:
```commandline
python3 preprocessDatabase.py --prep_sim_db --dataPath=simData
```
Note that the script acts in-place.
You might want to backup the original data for later use.

The resulting directory structure is as follows:
```
simData
├── test
│   ├── input
│   └── label
├── train
│   ├── input
│   └── label
└── valid
    ├── input
    └── label
```

#### Training
We assume data is under folder _simData_ and its structure matches the above described one.
Simulator baseline training can be reproduced using the following command:
```commandline
python3 RightLaneModule.py --gpus=1 --dataPath=simData --batch_size=64 --augment --reproducible --max_epochs=175
```

Consider changing CUDA and GPU dependent parameters to better utilize the hardware.
Other paramters are available for fine-tuning, see script or argument help for details.

### Domain Transformation using CycleGAN
This method uses the same database preparation and training process described in the baseline solution.
However it is preceded by a CycleGAN training step and before training simulator data should be converted to the real domain.
These two steps are described below.

#### Train CycleGAN
Clone an [implementation of CycleGAN](https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations/cyclegan).
From the provided repo we need _only_ the CycleGAN related parts, the others can be deleted.
In _cyclegan.py_ line 121 and 128 change "../../data/%s" to "%s".

Use [getRealData.py](rightLaneNetwork/utils/getRealData.py) to download real Duckietown images.
```commandline
python3 getRealData.py --num_images=-1 --savePath realData
```

The downloaded images will make up domain B while simulator images will be domain A.
First the simulator images can be obtained using [preprocessDatabase.py](rightLaneNetwork/utils/preprocessDatabase.py):
```commandline
python3 preprocessDatabase.py --prep_sim_db --dataPath simData2 --single_sim_dir
```

We are interested only in input images therefore _simData2/label_ folder can be discarded.
We would like to create the following directory structure, with leaf folders containing .png files:
```
sim2real
├── test
│   ├── A
│   └── B
└── train
    ├── A
    └── B
```

Assuming we have at least 11,500 simulator images and 30,000 real images in folders _simData2_ and _realData_ the following script divides them into the targeted structure:
```commandline
mkdir -p sim2real/test/A sim2real/test/B sim2real/train/A sim2real/train/B
find simData2/input/*.png | sort | head -n 10000 | shuf | head -n 5000 | xargs -I{} cp {} sim2real/train/A
find simData2/input/*.png | sort | tail -n 1500 | shuf | head -n 1500 | xargs -I{} cp {} sim2real/test/A
find realData/*.png | sort | head -n 25000 | shuf | head -n 5000 | xargs -I{} cp {} sim2real/train/B
find realData/*.png | sort | tail -n 5000 | shuf | head -n 1500 | xargs -I{} cp {} sim2real/test/B
```

Now train the CycleGAN (modify hyperparams if needed):
```commandline
python3 cyclegan.py --dataset_name sim2real --n_epochs 201 --batch_size 32 --n_cpu 8 --img_height 120 --img_width 160 --checkpoint_interval 25 --lambda_cyc 15 --lambda_id 10
```

Check the results and if satisfied copy _G_AB_200.pth_ to your working folder.

#### Convert simulator data to real domain
After creating and formatting the database use [sim2real_convert.py](rightLaneNetwork/utils/sim2real_convert.py) to transform simuator images to real domain.
This script converts the given database input images inplace.
```commandline
python3 sim2real_convert.py --dataPath simData --modelWeightsPath G_AB_200.pth
```

The segmentation model training process is the same as in the baseline solution. 

### SSDA via Minimax Entropy
#### Data preparation
For 30 images we have hand-made annotations.
Ask the repo maintainers for them.
Extract the acquired .zip file to _realData/annotated_.
Install python package _labelme_ to label images yourself and to create binary images from the created labels.
```commandline
pip install labelme
```

Run the bash script _json2imgs.sh_ in the unpacked zip file that converts the saved labels from json format to .png images.
Then the python script createRealDB.py creates the following directory structure from the available labelled and unlabelled data:
```
realData
├── input
├── label
└── unlabelled
```

To separate the obtained labelled images to separate train and test sets use the preprocessing script [preprocessDatabase.py](rightLaneNetwork/utils/preprocessDatabase.py)
```commandline
python3 preprocessDatabase.py --prep_real_db --dataPath realData
```

The above script created the following directory structure:
```
realData
├── test
│   ├── input
│   └── label
├── train
│   ├── input
│   └── label
└── unlabelled
    └── input
```

#### MME adapting a trained network
For SSDA MME a combined (but NOT merged) source-target database is required.
The following format is expected and can be created by copying the above databases:
```
dataSSDA
├── source
│   ├── input
│   └── label
└── target
    ├── test
    │   ├── input
    │   └── label
    ├── train
    │   ├── input
    │   └── label
    └── unlabelled
        └── input
```

The training can be done using the following command (see script and argument help for hyperparameters):
```commandline
python3 RightLaneMMEModule.py --gpus=1 --dataPath=dataSSDA --pretrained_path=results/baseline_weights.pth --batchSize=32 --augment --reproducible --max_epochs=175
```


## Testing
Trained model evaluation can be done using the provided script test.py.
The script takes command line arguments, see help for details.

Comparison of trained models can be done using comparison.py that generates an image file with sample predictions of each model.

One is able to make predictions for a video (stream of images) via makeDemoVideo.py.


## Known problems
- Distributed training is currently not working because of custom samplers in S&T and MME training.


## Good to know
For selecting GPU-s and Comet logging, the following environment variables and command line parameters have to be defined:
```commandline
COMET_API_KEY=your_api_key COMET_WORKSPACE=your_workspace COMET_PROJECT_NAME=your_project_name CUDA_VISIBLE_DEVICES=0 python3 ... --comet
```


## Links
- FC-DenseNet: [Paper](https://arxiv.org/abs/1611.09326), code copied from [here](https://github.com/bfortuner/pytorch_tiramisu)
- SSDA via MME: [Paper](https://arxiv.org/abs/1904.06487), [code](https://github.com/VisionLearningGroup/SSDA_MME)
